{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "    _   __                                  __           _   __         __                              __          \n",
        "   / | / /  ___   __  __   _____  ____ _   / /          / | / /  ___   / /_ _      __  ____    _____   / /__   _____\n",
        "  /  |/ /  / _ \\ / / / /  / ___/ / __ `/  / /          /  |/ /  / _ \\ / __/| | /| / / / __ \\  / ___/  / //_/  / ___/\n",
        " / /|  /  /  __// /_/ /  / /    / /_/ /  / /          / /|  /  /  __// /_  | |/ |/ / / /_/ / / /     / ,<    (__  )\n",
        "/_/ |_/   \\___/ \\__,_/  /_/     \\__,_/  /_/          /_/ |_/   \\___/ \\__/  |__/|__/  \\____/ /_/     /_/|_|  /____/  \n",
        "                                                                                                                           \n",
        "```                                                                           \n",
        "## Intro to Neural Networks and PyTorch\n",
        "\n",
        "What you will learn:\n",
        "- What is a neural network\n",
        "- Defining a neural network in numpy\n",
        "- Intro to PyTorch\n",
        "- Defining a neural network in PyTorch\n",
        "\n",
        "---\n",
        "\n",
        "What is a neural network?\n",
        "\n",
        "\n",
        "\n",
        "A neural network is designed to mimic the human brain, it is comprised of a series of neurons and layers which deal with inputs and produces some output.\n",
        "\n",
        "\n",
        "An example of a very basic neuron is as follows\n",
        "\n",
        "![](https://cdn.rawgit.com/leoloman/intro-pytorch-nn/main/perceptron.svg)\n",
        "\n",
        "\n",
        "This neuron has 2 inputs, $x_{1}$, $x_{2}$, which are fed into the weight layer, then this goes to a bias layer which is shown by the green layer, this is then followed by the activation function.\n",
        "\n",
        "What occurs in the network at each step is as follows:\n",
        "\n",
        "Red Layer:\n",
        "\n",
        "$$\n",
        "z_{1} = x_{1} * w_{1}\n",
        "$$\n",
        "$$\n",
        "z_{2} = x_{2} * w_{2}\n",
        "$$\n",
        "\n",
        "Green Layer:\n",
        "\n",
        "$$\n",
        "v = z_{1} + z_{2} + b\n",
        "$$\n",
        "\n",
        "Orage layer - activation function\n",
        "\n",
        "$$\n",
        "y = f(v)\n",
        "$$\n",
        "\n",
        "The activation function is used to convert the unbounded input into a predictable form, a commonly used activation function is the sigmoid function. The sigmoid function only outputs numbers within the range of 0 - 1, large negative numbers become close to 0 and large positive numbers become close to 1.\n",
        "\n",
        "Worked example:\n",
        "\n",
        "$$\n",
        "w = [0, 1]\n",
        "$$\n",
        "$$\n",
        "b = 4\n",
        "$$\n",
        "$$\n",
        "x = [2, 3]\n",
        "$$\n",
        "\n",
        "$$\n",
        "(0 * 2) + (1 * 3) + 4 = 7\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(7) = 0.999\n",
        "$$\n",
        "\n",
        "The process of passing inputs through a neuron to get an output is known as a feedforward.\n",
        "\n",
        "Now lets program this basic neural network\n"
      ],
      "metadata": {
        "id": "RYCqR_Jvhi4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hysQst9Whisp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"Activation function\n",
        "      f(x) = 1 / (1 + e^-x)\n",
        "  \"\"\"\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    return sigmoid(total)"
      ],
      "metadata": {
        "id": "qnYwygemy3OY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.array([0,1])\n",
        "bias = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2,3])\n",
        "print(n.feedforward(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_doYzFZ0z9v8",
        "outputId": "995128ba-a29a-47c7-a2e1-73492539e268"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBPc_DrYe9dv",
        "outputId": "5e561499-7121-4116-c350-ac1be3a89b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio torchvision torchtext torchdata transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "3wOizgGugDHe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network is a combination of multiple neurons connected together.\n",
        "\n",
        "Here is a simple neural network:\n",
        "![](https://cdn.rawgit.com/leoloman/intro-pytorch-nn/main/network.svg)\n",
        "\n",
        "This network has 2 inputs, a hidden layer with 2 neurons and an output layer with a single neuron.\n",
        "\n",
        "Lets take a worked example with the network, lets assume all neurons have the same weights and bias and the same sigmoid activation function.\n",
        "\n",
        "$$\n",
        "w = [0,1]\n",
        "$$\n",
        "$$\n",
        "b = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "h_{1} = h_{2} = f(w \\cdot x +b)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f((0 * 2) + (1*3) + 0) = 0.9526\n",
        "$$\n",
        "\n",
        "$$\n",
        "o_{1} = f(w \\cdot [h_{1}, h_{2}] + b)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(( 0 *0.9526) + ( 1 * 0.9526) + 0)\n",
        "$$\n",
        "\n",
        "$$\n",
        "f(0.9526) = 0.7216\n",
        "$$\n"
      ],
      "metadata": {
        "id": "wlTrE7obiSHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \"\"\"\n",
        "  A neural network with:\n",
        "    - 2 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "\n",
        "  Each neuron has the same weights and bias:\n",
        "    - w = [0,1]\n",
        "    - b = 0\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    weights = np.array([0,1])\n",
        "    bias = 0\n",
        "\n",
        "    self.h1 = Neuron(weights, bias)\n",
        "    self.h2 = Neuron(weights, bias)\n",
        "    self.o1 = Neuron(weights, bias)\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    out_h1 = self.h1.feedforward(x)\n",
        "    out_h2 = self.h2.feedforward(x)\n",
        "\n",
        "    # The inputs for o1 are the outputs from h1 and h2\n",
        "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "\n",
        "    return out_o1\n",
        "\n",
        "network = NeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(network.feedforward(x))\n"
      ],
      "metadata": {
        "id": "UiX0uPBKiK2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43131ba0-afb2-45b4-d231-31bce4d2c09f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7216325609518421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a neural network"
      ],
      "metadata": {
        "id": "gk2_CWpt4QL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_train = {'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "                 'weight': [133, 160, 152, 120],\n",
        "                 'height':[65, 72, 70, 60],\n",
        "                 'sex' : ['F', 'M', 'M', 'F']}"
      ],
      "metadata": {
        "id": "EPLBlEM34Ofp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_train['weight'] = [(x - np.mean(data_to_train['weight']))/np.std(data_to_train['weight']) for x in data_to_train['weight']]\n",
        "data_to_train['height'] = [(x - np.mean(data_to_train['height']))/np.std(data_to_train['height']) for x in data_to_train['height']]\n",
        "data_to_train['sex'] = [1 if x == 'F' else 0 for x in data_to_train['sex']]"
      ],
      "metadata": {
        "id": "B7lZ7EbJ4Pal"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a neural network\n",
        "\n",
        "When training a neural network you can decide to use various metrics to define how well your neural network fits to data, this is defined as Loss.\n",
        "\n",
        "In this case we will use the Mean Squared Error to fit the neural network to data\n",
        "\n",
        "The mean squared error is defined as\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum(y_{true} - y_{pred})^2\n",
        "$$\n",
        "\n",
        "Where broken down:\n",
        "- $n$ is the number of samples in your data\n",
        "- $y$ represents an observed value\n",
        "- $y_{true}$ is your observed data\n",
        "- $y_{pred}$ is your neural networks predicted value\n",
        "\n",
        "$(y_{true} - y_{pred})^2$ is the squared error between the observed and predicted value, then you simply take the mean of all of these."
      ],
      "metadata": {
        "id": "KpZ-ZC8ZSX6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Mean squared error\n",
        "  \"\"\"\n",
        "  return np.mean(np.power(y_true - y_pred, 2))"
      ],
      "metadata": {
        "id": "gKcBzzNwSXy3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function can be written as a muli-variable function:\n",
        "\n",
        "$$\n",
        "L(w_{1}, w_{2}, w_{3}, w_{4}, w_{5}, w_{6}, b_{1}, b_{2}, b_{3})\n",
        "$$\n",
        "\n",
        "Now imagine we want to tweak $w_{1}$, how would the Loss change if we changed $w_{1}$? The partial derivative $\\frac{\\partial L}{\\partial w_{1}}$ can answer.\n",
        "\n",
        "To start we will write the partial derivative in terms of $\\frac{\\partial y_{pred}}{\\partial w_{1}}$ instead:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{1}} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial w_{1}}\n",
        "$$\n",
        "\n",
        "We can calculate $\\frac{\\partial L}{\\partial y_{pred}}$  as $L = \\sum(y_{true} - y_{pred})^2$\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_{pred}} = \\frac{\\partial (y_{true} - y_{pred})^2}{\\partial y_{pred}} = -2(y_{true} - y_{pred})\n",
        "$$\n",
        "\n",
        "Now we need to break down\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y_{pred}}{\\partial w_{1}}\n",
        "$$\n",
        "\n",
        "Now $h_{1}, h_{2}, o_{1}$ will represent their neurons then\n",
        "\n",
        "$$\n",
        "y_{pred} = o_{1} = f(w_{5} h_{1} + w_{6} h_{2} +b_{3})\n",
        "$$\n",
        "\n",
        "Where $f$ is the sigmoid activation function.\n",
        "\n",
        "Since $w_{1}$ only affects $h_{1}$ not $h_{2}$, we can write.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y_{pred}}{\\partial w_{1}} = \\frac{\\partial y_{pred}}{\\partial h_{1}} * \\frac{\\partial h_{1}}{\\partial w_{1}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y_{pred}}{\\partial h_{1}} = w_{5} * f'(w_{5} h_{1} + w_{6} h_{2} + b_{3})\n",
        "$$\n",
        "\n",
        "Then we will do the same thing for $\\frac{\\partial h_{1}}{\\partial w_{1}}$:\n",
        "\n",
        "$$\n",
        "h_{1} = f(w_{1} x_{1} + w_{2} x_{2} + b_{1})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h_{1}}{\\partial w_{1}} = x_{1} * f'(w_{1} x_{1} + w_{2} x_{2} + b_{1})\n",
        "$$\n",
        "\n",
        "Where $x_{1}$ is the weight and $x_{2}$ is the height. Included in the above equations is $f'$ which is the derivative of the sigmoid function:\n",
        "\n",
        "$$\n",
        "f'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
        "$$\n",
        "\n",
        "Now $\\frac{\\partial L}{\\partial w_{1}}$ has been broken down into several manageable parts which can be calculated.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_{1}} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_{1}} * \\frac{\\partial h_{1}}{\\partial w_{1}}\n",
        "$$\n",
        "\n",
        "Keep in mind this was just for altering $w_{1}$ but the process would be similar for the subsequent values. The system for calculating these partial derivatives by working backwards is known as backpropagation or 'backprop'.\n",
        "\n",
        "Training: Stochastic Gradient Descent\n",
        "\n",
        "In the following example we will use a method of training called 'Stochastic Gradient Descent' but there are many different ways of training neural networks. SGD tells us how to change the weights and bias to minimize the loss. There is the following update equation:\n",
        "\n",
        "$$\n",
        "w_{1} \\leftarrow w_{1} - \\eta \\frac{\\partial L}{\\partial w_{1}}\n",
        "$$\n",
        "\n",
        "$\\eta$ is a constant value called the learning rate which controls how fast the model can be trained. The above term is subtracted from $w_{1}$, if the above term is positive  then $w_{1}$ will decrease which will make $L$ decrease. If negative then $w_{1}$ will increase and so will $L$.\n",
        "\n",
        "\n",
        "The training process is as follows:\n",
        "1. Choose a sample from the dataset.\n",
        "2. Calculate all the partial derivatives of loss with respect to weight or bias\n",
        "3. Use the update equation to update the weight or bias\n",
        "4. Go back to step 1 until either convergence or a set number of epochs has been met."
      ],
      "metadata": {
        "id": "uPOsKZTHVI1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deriv_sigmoid(x):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "class NeuralNetwork:\n",
        "  '''\n",
        "  A neural network with:\n",
        "    - 2 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "\n",
        "  *** DISCLAIMER ***:\n",
        "  The code below is intended to be simple and educational, NOT optimal.\n",
        "  Real neural net code looks nothing like this. DO NOT use this code.\n",
        "  Instead, read/run it to understand how this specific network works.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    # Weights\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Biases\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x is a numpy array with 2 elements.\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    '''\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
        "    - all_y_trues is a numpy array with n elements.\n",
        "      Elements in all_y_trues correspond to those in data.\n",
        "    '''\n",
        "    learn_rate = 0.1\n",
        "    epochs = 1000 # number of times to loop through the entire dataset\n",
        "    self.loss = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "\n",
        "        loss = mse(all_y_trues, y_preds)\n",
        "        if epoch % 100 == 0:\n",
        "          print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
        "        self.loss.append([epoch, loss])\n",
        "\n",
        "# prompt: can you reshape data_to_train into a matrix, can you use base python\n",
        "\n",
        "data_matrix = []\n",
        "for i in range(len(data_to_train['name'])):\n",
        "  row = np.array([data_to_train['weight'][i], data_to_train['height'][i]])\n",
        "  data_matrix.append(row)\n",
        "\n",
        "# Define dataset\n",
        "data = np.array(data_matrix)\n",
        "all_y_trues = np.array(data_to_train['sex'])\n",
        "\n",
        "\n",
        "# Train our neural network!\n",
        "network = NeuralNetwork()\n",
        "network.train(data, all_y_trues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYkxsSuX5b2K",
        "outputId": "adf297bf-3fc9-438e-c9db-2fa57bd647c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.365\n",
            "Epoch 100 loss: 0.090\n",
            "Epoch 200 loss: 0.035\n",
            "Epoch 300 loss: 0.020\n",
            "Epoch 400 loss: 0.013\n",
            "Epoch 500 loss: 0.009\n",
            "Epoch 600 loss: 0.007\n",
            "Epoch 700 loss: 0.006\n",
            "Epoch 800 loss: 0.005\n",
            "Epoch 900 loss: 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "YKBY2ir9Yxt3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([x[0] for x in network.loss], [x[1] for x in network.loss])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "dt6MJNnCYyGC",
        "outputId": "7044b279-e947-4355-f027-b786f27ffe1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions\n",
        "jessica = np.array([-7, -3]) # 128 pounds, 63 inches\n",
        "adam = np.array([20, 2])  # 155 pounds, 68 inches\n",
        "print(\"Jessica: %.3f\" % network.feedforward(jessica)) # F\n",
        "print(\"Adam: %.3f\" % network.feedforward(adam)) # M"
      ],
      "metadata": {
        "id": "SsaY0v4TiQ0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ad8a70-545c-4bc1-e752-c83ad52c80d1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jessica: 0.947\n",
            "Adam: 0.028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "\n",
        "PyTorch has become the industry standard package for developing and loading and running inference with neural networks / large language models\n",
        "\n",
        "\n",
        "https://pytorch.org/"
      ],
      "metadata": {
        "id": "9pI1Lz9XjdNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa6GQzYudxkF",
        "outputId": "8546c4ef-5141-462c-b7a5-c6ad90312da8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 15 09:16:41 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "x6d6mf95iKWe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch allows the user to use different devices:\n",
        "- \"cuda\" - Nvidia GPU\n",
        "- \"mps\" - Apple Metal Performance Shaders (m series macs)\n",
        "- \"cpu\""
      ],
      "metadata": {
        "id": "OEGUrj85joji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available() # this checks whether a gpu is available\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available() # this is for apple m1 chips\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNko1LmCgEjQ",
        "outputId": "880c4474-b255-46fb-d782-a6caa8fe8489"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJPFUFUScNx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Tensors to hold input and outputs.\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
        "# we can consider it as a linear layer neural network. Let's prepare the\n",
        "# tensor (x, x^2, x^3).\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)\n",
        "\n",
        "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
        "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
        "# of shape (2000, 3)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. The Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
        "# to match the shape of `y`.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(3, 1),\n",
        "    torch.nn.Flatten(0, 1)\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(xx)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "# You can access the first layer of `model` like accessing the first item of a list\n",
        "linear_layer = model[0]\n",
        "\n",
        "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
        "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
      ],
      "metadata": {
        "id": "7zrKarRxhIno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b67460a-fa14-4f92-fe0b-c9dea8ddd224"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 265.8453674316406\n",
            "199 179.88084411621094\n",
            "299 122.7143325805664\n",
            "399 84.68437957763672\n",
            "499 59.37547302246094\n",
            "599 42.52561569213867\n",
            "699 31.30276107788086\n",
            "799 23.824373245239258\n",
            "899 18.838787078857422\n",
            "999 15.513387680053711\n",
            "1099 13.294169425964355\n",
            "1199 11.81234073638916\n",
            "1299 10.822325706481934\n",
            "1399 10.160476684570312\n",
            "1499 9.71776008605957\n",
            "1599 9.421399116516113\n",
            "1699 9.22288703918457\n",
            "1799 9.089813232421875\n",
            "1899 9.000540733337402\n",
            "1999 8.940607070922852\n",
            "Result: y = -0.006111369468271732 + 0.8475133776664734 x + 0.0010543126845732331 x^2 + -0.09201786667108536 x^3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Large Language Model onto GPU"
      ],
      "metadata": {
        "id": "Yz8X1DRBgbfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
        "import transformers"
      ],
      "metadata": {
        "id": "MpX3gPDYe87-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "#                                              quantization_config=BitsAndBytesConfig(load_in_8bit=True),  # Enable 8-bit quantization\n",
        "#                                              device_map=\"auto\",  # Auto-assign GPUs\n",
        "#                                              trust_remote_code=True)\n",
        "\n",
        "# falcon_pipeline = transformers.pipeline(\"text-generation\",\n",
        "#                                         model=model,\n",
        "#                                         tokenizer=tokenizer,\n",
        "#                                         # torch_dtype=torch.bfloat16,\n",
        "#                                         # trust_remote_code=True,\n",
        "#                                         # device_map=\"auto\"\n",
        "#                                         )"
      ],
      "metadata": {
        "id": "r9IjCH-3v6Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "falcon_pipeline = transformers.pipeline(\"text-generation\",\n",
        "                                        model=model,\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        torch_dtype=torch.bfloat16,\n",
        "                                        trust_remote_code=True,\n",
        "                                        device_map=\"auto\"\n",
        "                                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "O_oLTxSxfQWp",
        "outputId": "1297bbe8-6708-4adb-dff9-08187958b629"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ff034eb19727>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(model_name,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                              \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Enable 8-bit quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Auto-assign GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    560\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3399\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "YXYfpLXjfXTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_completion_falcon(input):\n",
        "  system = f\"\"\"\n",
        "  You are an expert Physicist.\n",
        "  You are good at explaining Physics concepts in simple words.\n",
        "  Help as much as you can.\n",
        "  \"\"\"\n",
        "  prompt = f\"#### System: {system}\\n#### User: \\n{input}\\n\\n#### Response from falcon-7b-instruct:\"\n",
        "  print(prompt)\n",
        "  falcon_response = falcon_pipeline(prompt,\n",
        "                                    max_length=500,\n",
        "                                    do_sample=True,\n",
        "                                    top_k=10,\n",
        "                                    num_return_sequences=1,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,\n",
        "                                    )\n",
        "  return falcon_response"
      ],
      "metadata": {
        "id": "BelKXy8IfShe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain to me the difference between nuclear fission and fusion.\"\n",
        "response = get_completion_falcon(prompt)\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w9-7uMgfS8J",
        "outputId": "bf543ff6-b565-4b60-c409-c473502b08e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### System: \n",
            "  You are an expert Physicist.\n",
            "  You are good at explaining Physics concepts in simple words.\n",
            "  Help as much as you can.\n",
            "  \n",
            "#### User: \n",
            "Explain to me the difference between nuclear fission and fusion.\n",
            "\n",
            "#### Response from falcon-7b-instruct:\n",
            "#### System: \n",
            "  You are an expert Physicist.\n",
            "  You are good at explaining Physics concepts in simple words.\n",
            "  Help as much as you can.\n",
            "  \n",
            "#### User: \n",
            "Explain to me the difference between nuclear fission and fusion.\n",
            "\n",
            "#### Response from falcon-7b-instruct:\n",
            "#### Answer:\n",
            "\n",
            "The main difference between nuclear fission and fusion is the type of nuclear reaction they produce. In nuclear fission, an unstable, heavy nucleus splits into two or more smaller nuclei and releases a significant amount of energy in the form of gamma radiation and free neutrals. On the other hand, fusion is a nuclear reaction in which two lighter nuclei combine to form a heavier nucleus, releasing a substantial amount of energy in the process. Fusion releases energy due to the binding energy of the nucleus, which holds the nuclei together.\n",
            "\n",
            "In summary, while both fusion and fission involve nuclear reactions, fusion creates heavier nuclei with more neutrons, releasing more energy and fission just releases energy.\n",
            "\n",
            "Hope this helps explain nuclear reactions to you! Let me know if you have any more questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing"
      ],
      "metadata": {
        "id": "lqvW2cQRe8Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pd9TJbMLe7Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oWBMOcPJbx0-"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}